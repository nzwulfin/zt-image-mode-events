= Deploy a virtual machine from a bootc image

In this exercise, you will build a `qcow2` disk image from the bootc image and then launch
a virtual machine from it. `qcow2` is a standard file format used by the Linux virtualization system.

[#config]
== Create the config for building the virtual machine image

To deploy our bootc image as a host, we need to get the contents of the image onto a disk. While `bootc` 
handles those mechanics, the actual creation of a physical disk, virtual disk, or cloud image are handled 
by other standard tools. For example, Anaconda can be used with bootc images for physical or virtual hosts 
like you would today. 

In this lab, we'll use the `bootc-image-builder` tool which can create various different disk image types and call `bootc` to deploy the image contents. This is a variant of the Image Builder tooling you are already familiar with, just containerized and with `bootc` built in.

You may have noticed the bootc image we've created does not include any login credentials. Not a 
typical concern for an application container, but as a host we will very likely need to interact
at some point. Since we are defining an image to be shared by multiple hosts in multiple different environments,
users and authentication is likely to be handled at the deployment stage, not the build stage.

TIP: There are cases where it may be useful for a user and credentials to be added to an image, 
as a 'break glass' emergency login for example.

To add a user during the deployment to a disk image, the credentials are put into the config file used by `bootc-image-builder` to create the final disk image.

An ssh key pair was generated for use during this lab, you can view the public part like this for later use. There's no newline at the end of this file, so the `echo` is just to make copying the output easier.
[source,bash,role="execute",subs=attributes+]
----
cat ~/.ssh/{guid}key.pub; echo
----
....
ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAuXnpoluye+KM+9tvIAdHf+F0IHh+K73tlcjEG8LJRB
....
NOTE: This is only a sample and this public key will not work in your environment

You can now create a file called `config-qcow2.toml` with the following contents, replacing "SSHKEY" 
in the `key` field with the contents of public key from your system.

[source,bash,role="execute",subs=attributes+]
----
nano config.toml
----

[source,yaml,role="execute",subs=attributes+]
----
[[customizations.user]]
name = "core"
password = "redhat"
groups = ["wheel"]
key = "SSHKEY"
----

This configuration blueprint creates a user in the wheel group with the login `core`, the password `redhat` and the ssh key of the user on the lab host.

[#create]
== Create the disk image

To create a bootable disk image from an OCI image, we have a special version of image builder that has support for `bootc`. This `bootc-image-builder` itself runs as a container, and as a result needs additional capabilities and to be run as `root`. 

As `bootc-image-builder` accesses local storage to find the source image, we will need to copy our image from user storage to system storage. The `podman image scp` command can do this for us, as well as copying to various different targets. When executed with a source but without a destination as below, the default is local system storage.


Try to generate the image now:

NOTE: This could take 2+ minutes to complete.

[source,bash,role="execute",subs=attributes+]
----
sudo podman run --rm --privileged --security-opt label=type:unconfined_t \
  --volume ./config.toml:/config.toml \
  --volume /var/lib/containers/storage:/var/lib/containers/storage \
  --volume .:/output \
  registry.redhat.io/rhel10/bootc-image-builder:10.1 \
  --type qcow2 \
  registry-{guid}.{domain}/base
----

A brief explanation of the arguments used for `podman run` and `bootc-image-builder`:

  * `--privileged` -> add capabilities required to build the image
  * `--volume` -> podman will map these local directories or files to the container to capture the config, load the bootc image, and store the results
  * `registry.redhat.io/rhel10/bootc-image-builder:10.1` -> the image builder container image

After the image builder version, these arguments passed to `bootc-image-builder`:

  * `--type qcow2` -> the type of image to build
  * `registry-{guid}.{domain}/base` -> the bootc image we are unpacking into the qcow2 disk

Once this completes, you will find the image in the `qcow2/` directory:

[source,bash,role="execute",subs=attributes+]
----
file qcow2/disk.qcow2
----
....
qcow2/disk.qcow2: QEMU QCOW2 Image (v3), 10737418240 bytes
....

[#create-vm]
== Create the virtual machine

Since we provided the QCOW2 type to `bootc-image-builder`, the resulting disk image is complete and ready to run without additional installation or other steps. We can copy the image to the standard storage pool location for KVM.

[source,bash,role="execute",subs=attributes+]
----
cp qcow2/disk.qcow2 /var/lib/libvirt/images/bootc-vm.qcow2
----

The creation of KVM virtual machines is out of scope for this lab, but the core of the `virt-install` command used is `--import` which skips any install process and creates the VM around the provided disk image. 

[source,bash,role="execute",subs=attributes+]
----
virt-install --connect qemu:///system \
  --name bootc-vm \
  --disk /var/lib/libvirt/images/bootc-vm.qcow2 \
  --import \
  --memory 4096 \
  --graphics none \
  --osinfo rhel10-unknown \
  --noautoconsole \
  --noreboot
----

We can now start our new bootc virtual machine.

[source,bash,role="execute",subs=attributes+]
----
virsh --connect qemu:///system start bootc-vm
----

Check to make sure the virtual machine running:

[source,bash,role="execute",subs=attributes+]
----
virsh --connect qemu:///system list
----
....
 Id   Name                State
------------------------------------
 1    bootc-vm                running
....

[#test]
== Test and login to the virtual machine

[source,bash,role="execute",subs=attributes+]
----
btop
----

[source,bash,role="execute",subs=attributes+]
----
sudo bootc status
----
....
 Booted image: registry-mjghb.apps.ocpvdev01.rhdp.net/base
        Digest: sha256:b937c4633ae298d3c68d97810b1b6dc276e8d47f8aea0a6ba85eff540b966539 (amd64)
       Version: 10.1 (2025-11-13T16:56:36Z)
....
<1> This section details the name of the image, the version with creation timestamp, and the SHA256 digest from the registry. This digest should match the previous output from `skopeo`. 

[source,bash,role="execute",subs=attributes+]
----
sudo bootc status --format yaml
----
....
apiVersion: org.containers.bootc/v1
kind: BootcHost
metadata:
  name: host
spec:
  image:
    image: registry-mjghb.apps.ocpvdev01.rhdp.net/base
    transport: registry
  bootOrder: default
status:
  staged: null
  booted:
    image:
      image:
        image: registry-mjghb.apps.ocpvdev01.rhdp.net/base
        transport: registry
      version: '10.1'
      timestamp: 2025-11-13T16:56:36.232037898Z
      imageDigest: sha256:b937c4633ae298d3c68d97810b1b6dc276e8d47f8aea0a6ba85eff540b966539
      architecture: amd64
    cachedUpdate: null
    incompatible: false
    pinned: false
    softRebootCapable: true
    store: ostreeContainer
    ostree:
      stateroot: default
      checksum: 7b67791e3f5f79e86d9d4f297cfbac57de6e96f67df46622047429bf60429b99
      deploySerial: 0
  rollback: null
  rollbackQueued: false
  type: bootcHost
....

This status provides information about the images on the host. There are 3 different images that may be available on a bootc host: the booted image, the staged image, and the rollback image. We'll discuss the latter two images later in the lab. The booted image is what's currently defined as the active environment. The image name here is what `bootc` tracks to detect any updates that come available. 


First, `bootc` tells you directly if it's being run on an image mode host or not. If `bootc` were to be installed and run on a non-bootc host, `bootc status` will show all `null` values instead of the output seen here. 

For other ways, we can look at how the system was started, let's look at kernel command line.

[source,bash,role="execute",subs=attributes+]
----
 cat /proc/cmdline
----
....
BOOT_IMAGE=(hd0,gpt3)/boot/ostree/default-8229ec6e3d87cd7730bff98dd2a6aeedb8727371cd61de8783c3fe8234bd2797/vmlinuz-6.12.0-124.8.1.el10_1.x86_64 root=UUID=37979fd9-1866-466d-8795-4c8c7594031c rw boot=UUID=5613f646-1ab4-49cd-862b-c0636c26af6a rw console=tty0 console=ttyS0 ostree=/ostree/boot.1/default/8229ec6e3d87cd7730bff98dd2a6aeedb8727371cd61de8783c3fe8234bd2797/0
....

We can see in the kernel command line some clear ties to an `ostree` partition, which is how images are stored and managed on a bootc host. We'll talk more about that later.

One other obvious difference for bootc hosts is the layout of the filesystem. 

[source,bash,role="execute",subs=attributes+]
----
 df -Th
----
....
Filesystem     Type      Size  Used Avail Use% Mounted on
composefs      overlay   7.6M  7.6M     0 100% /
/dev/vda4      xfs       8.5G  2.3G  6.3G  27% /etc
devtmpfs       devtmpfs  1.9G     0  1.9G   0% /dev
tmpfs          tmpfs     2.0G     0  2.0G   0% /dev/shm
tmpfs          tmpfs     782M  608K  782M   1% /run
tmpfs          tmpfs     2.0G     0  2.0G   0% /tmp
tmpfs          tmpfs     1.0M     0  1.0M   0% /run/credentials/systemd-journald.service
/dev/vda3      xfs       960M  157M  804M  17% /boot
/dev/vda2      vfat      501M  8.8M  492M   2% /boot/efi
tmpfs          tmpfs     391M  4.0K  391M   1% /run/user/1000
tmpfs          tmpfs     1.0M     0  1.0M   0% /run/credentials/getty@tty1.service
tmpfs          tmpfs     1.0M     0  1.0M   0% /run/credentials/serial-getty@ttyS0.service
....

Rather than the usual layout, you'll notice that our root filesystem is an overlay and `/sysroot` looks like where most of the storage is used. You can also check the output of `ls -l /` and notice that there are a lot of symlinks where you might expect directories or filesystems.  This is a key difference that is tied to how updates, rollbacks, and software management works on image mode hosts.  We'll explore this in a later exercise.

[source,bash,role="execute",subs=attributes+]
----
systemctl status rsyslog.service
----
Before proceeding, make sure you have changed back to the builder terminal.

